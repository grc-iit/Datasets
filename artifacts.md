# Comprehensive HPC and Scientific Computing Dataset Repository Guide

**The HPC dataset landscape offers petabytes of publicly accessible data across formats like HDF5, ADIOS, and Parquet.** This comprehensive guide identifies 60+ major datasets and repositories spanning I/O traces, climate modeling, astrophysics, molecular dynamics, CFD, and materials science—all maintained by authoritative sources like national laboratories, NASA, CERN, and major research institutions. These datasets serve dual purposes: enabling domain scientists to conduct cutting-edge research while providing HPC developers with real-world data for testing I/O systems and storage infrastructure.

The datasets range from continuously updated production I/O traces (1.2M+ Darshan logs from ALCF) to massive simulation archives (IllustrisTNG's 1.1 petabyte cosmology database), offering researchers immediate access to high-quality scientific data without requiring supercomputer access for data generation.

## HPC file format datasets: ADIOS, HDF5, and specialized I/O formats

The HPC community has developed specialized file formats optimized for parallel I/O and large-scale scientific computing. **ADIOS2 (Adaptable I/O System) stands as the premier HPC-native format**, designed specifically for exascale computing with support for streaming workflows and in-situ analysis. The ADIOS2 Examples Repository at github.com/ornladios/ADIOS2-Examples provides comprehensive application examples demonstrating real-world usage patterns across C++, C, Python, and Fortran. Oak Ridge National Laboratory maintains this actively developed resource with extensive documentation at adios2.readthedocs.io.

**For production scientific workflows using ADIOS**, the PIConGPU Streaming Data Pipeline dataset (zenodo.org/records/4906276, DOI: 10.5281/zenodo.4906276) offers petascale plasma physics simulation data from the Summit supercomputer. This dataset showcases ADIOS2's streaming capabilities using the openPMD (Open Standard for Particle-Mesh Data) metadata standard, making it valuable for both domain scientists and HPC developers testing streaming I/O systems. The openPMD-api repository (github.com/openPMD/openPMD-api) provides tools for working with both ADIOS2 and HDF5 files following this standardized format.

**HDF5 dominates the scientific computing landscape** as the de facto standard for hierarchical scientific data storage. NASA has standardized on HDF5 across its Earth science missions, with the HDF-EOS5 format used by major satellites including OCO-2, JPSS, Aqua, Aura, and the Global Precipitation Measurement mission. The NASA Earthdata portal (earthdata.nasa.gov) provides access to petabytes of Earth science data in HDF5/HDF-EOS5 formats through data centers like GES DISC (disc.gsfc.nasa.gov) and PO.DAAC (podaac.jpl.nasa.gov), including ocean salinity measurements, precipitation data, and atmospheric composition datasets.

Climate science extensively uses **NetCDF-4, which is built on the HDF5 data model**, for storing simulation and reanalysis data. The massive ERA5 reanalysis dataset from ECMWF (rda.ucar.edu/datasets/d633006) contains 390 terabytes of hourly atmospheric data from 1950-present in NetCDF-4/HDF5 format with CF conventions. NCAR's CESM Large Ensemble (registry.opendata.aws/ncar-cesm-lens) provides 500 TB of climate model output, with a 70 TB subset available on AWS S3 in both NetCDF and Zarr formats. The CMIP6 climate model intercomparison project (esgf-node.llnl.gov/projects/cmip6) distributes hundreds of terabytes across the Earth System Grid Federation, all in NetCDF format following standardized conventions.

**For molecular dynamics, specialized HDF5 formats** have emerged as standards. The mdCATH dataset (open.playmolecule.org/mdcath) contains over 3 TB of all-atom MD simulations for 5,398 protein domains, storing 134,950 trajectories in HDF5 format with coordinates, forces, and structural properties. MDTraj (mdtraj.org) provides both a Python library and HDF5 format specification widely adopted in the MD community. The H5MD format (nongnu.org/h5md) defines a standardized HDF5 structure supported by multiple MD packages including LAMMPS and HOOMD.

**Parquet adoption in scientific HPC remains limited** but is growing in machine learning applications. The AirfRANS aerodynamics dataset (data.isir.upmc.fr/extrality/NeurIPS_2022) provides CFD simulation results in Parquet format optimized for ML workflows, with pressure and velocity fields for airfoils at various angles of attack. The format's columnar storage proves advantageous for large-scale data analytics but lacks the parallel I/O capabilities and scientific metadata standards that make HDF5/NetCDF preferred for traditional HPC applications.

## I/O traces and benchmarks: Characterizing HPC storage performance

**Production I/O traces provide invaluable insights into real-world HPC application behavior.** The ALCF Polaris Darshan Log Collection (zenodo.org/records/15353810) represents the most comprehensive and current resource, continuously updated daily with anonymized production logs from all jobs on Argonne's 560-node Polaris system. As of June 2025, this collection contains over 1.2 million log files growing at approximately 3,000 logs per day, making it essential for understanding modern HPC I/O patterns. Each Darshan log captures detailed I/O operation statistics including file access patterns, bandwidth achieved, and POSIX/MPI-IO operation counts.

**Historical trace collections** complement current data with long-term workload characterization. The Blue Waters Darshan Dataset (bluewaters.ncsa.illinois.edu/data-sets, accessed via Globus) provides large-scale traces from NCSA's decommissioned Cray XE6/XK7 system, including system monitoring metrics and Lustre user experience data. ALCF's Mira I/O Data Repository (ftp.mcs.anl.gov/pub/darshan/data) contains traces from the IBM Blue Gene/Q system. For learning specific I/O patterns, the Darshan Example Logs Repository (github.com/darshan-hpc/darshan-logs) curates notable logs demonstrating various corner cases with comprehensive README descriptions.

**Application-level I/O tracing at multiple API layers** enables deeper analysis than Darshan alone. The HPC Application I/O Traces Dataset from UC San Diego (library.ucsd.edu/dc/object/bb95276921, DOI: 10.6075/J0Z899X4) contains comprehensive trace files from 17+ HPC simulation applications recorded by the Recorder framework. These traces capture every I/O operation across HDF5, MPI-IO, and POSIX layers with all parameters, spanning scales from 1,024+ process executions. Applications traced include HACC-IO, MILC-QCD, pF3D-IO, and VPIC-IO, with multi-gigabyte datasets per application stored in binary .ift and .mt formats parseable via Recorder tools (github.com/uiuc-hpc/Recorder).

**Storage system traces from production HPC facilities** help researchers understand file system behavior. The SNIA IOTTA Repository (iotta.snia.org/traces) serves as a worldwide repository for storage-related traces, including parallel traces from supercomputers, HPC summaries with statistical data, NFS traces, and system call traces. While some HPC traces exceed 10 years in age, the repository provides valuable historical workload data. The LANL HPC Failure Data (usenix.org/cfdr-data) includes disk failure records and hardware replacement logs from large-scale systems, essential for storage reliability research.

**Benchmark tools and standardized results** enable performance comparisons across systems. The IOR benchmark (github.com/hpc/ior, ior.readthedocs.io) has become the de facto standard for parallel I/O testing, supporting POSIX, MPI-IO, HDF5, HDFS, S3, NCMPI, and RADOS APIs. The IO500 benchmark (io500.org, github.com/IO500/io500) provides a comprehensive storage benchmark suite with a public results database updated at ISC and SC conferences, covering both production and research system categories with IOR bandwidth tests and mdtest metadata operations.

## Climate and earth sciences: Weather models and reanalysis datasets

Climate modeling generates some of the largest publicly accessible scientific datasets. **CMIP6 coordinates global climate projections from 23+ general circulation models** under common experimental protocols, providing the scientific foundation for IPCC assessment reports. The data, distributed via the Earth System Grid Federation (esgf-node.llnl.gov/projects/cmip6), spans hundreds of terabytes across federated archives in the USA, France, Germany, and UK. All data uses NetCDF format following CF conventions, with daily and monthly gridded output for temperature, precipitation, and dozens of other climate variables under historical and future scenarios. Access methods include federated search portals, wget, Globus transfers, and Python APIs. NASA's NEX-GDDP-CMIP6 (nccs.nasa.gov/services/data-collections/land-based-products/nex-gddp-cmip6, also on AWS) provides high-resolution 0.25° downscaled and bias-corrected projections totaling multiple terabytes.

**Reanalysis datasets combine observations with models** to produce consistent historical climate records. ERA5 from ECMWF represents the gold standard, with 31km resolution and 137 vertical levels covering 1950-present at hourly frequency. NCAR hosts the complete model-level ERA5 data (rda.ucar.edu/datasets/d633006) totaling approximately 390 TB in netCDF-4/HDF5 format with CF 1.6 compliance. Alternative access includes AWS S3 (registry.opendata.aws/ecmwf-era5) and the Copernicus Climate Data Store (cds.climate.copernicus.eu). NOAA's NCEP/NCAR Reanalysis (psl.noaa.gov/data/gridded/data.ncep.reanalysis.html) provides an earlier but widely-used dataset from 1948-present at 2.5° resolution in NetCDF format.

**NASA Earth science data centers** maintain petabyte-scale archives across multiple disciplines. The Goddard Earth Sciences Data and Information Services Center (disc.gsfc.nasa.gov) hosts atmospheric composition, precipitation, and solar irradiance data in HDF5 and HDF-EOS5 formats. Key datasets include GPM Level 1-3 precipitation data in HDF5, AIRS atmospheric sounding in HDF-EOS5 grids, and the MERRA-2 reanalysis. The Physical Oceanography DAAC (podaac.jpl.nasa.gov) distributes ocean and hydrology datasets including Aquarius sea surface salinity in HDF5, SWOT surface water measurements, and GRACE gravity data. Both centers provide web portals, OPeNDAP access, and AWS S3 distribution for their multi-terabyte collections.

**Regional and high-resolution modeling** complements global datasets. NCAR's Community Earth System Model (CESM) Large Ensemble (registry.opendata.aws/ncar-cesm-lens) contains a 40-member ensemble of climate simulations from 1920-2100 under the RCP8.5 scenario, totaling 500 TB with a 70 TB subset on AWS S3 in Zarr format for cloud-optimized access. The dataset includes 2D and 3D variables across atmosphere, ocean, land, and ice domains. NCAR's Nested Regional Climate Model using WRF (rda.ucar.edu/datasets/ds601.0) provides 2.14 TB of regional simulations at 3-hourly and 6-hourly resolution for North America, Europe, and oceanic regions.

## Astrophysics and cosmology: Simulating the universe

**Cosmological hydrodynamical simulations** capture galaxy formation and evolution across cosmic time. IllustrisTNG (tng-project.org/data) stands as one of the most comprehensive public simulation suites, with three volumes (TNG50, TNG100, TNG300) at varying resolutions totaling 1.1 petabytes of data online. The project provides 2,000 full volume snapshots and approximately 110,000 high time-resolution subbox snapshots across 100 redshifts, all stored in HDF5 format with Python, IDL, and MATLAB helper scripts. The simulations track dark matter, gas, stars, and black holes with halo/subhalo catalogs and merger trees, accessible through both direct download and a web-based API with JupyterLab interface for browser-based analysis.

**CAMELS (Cosmology and Astrophysics with Machine Learning Simulations)** specifically targets machine learning applications with 4,233 cosmological simulations systematically varying cosmological and astrophysical parameters. The complete dataset (camel-simulations.org/data) spans over 200 TB stored in HDF5 format, comprising 1,092 IllustrisTNG simulations plus 1,092 SIMBA simulations with N-body counterparts, totaling 144,840 snapshots with SUBFIND halo catalogs. The CAMELS Multifield Dataset (camels-multifield-dataset.readthedocs.io) expands this with over 70 TB of 2D maps (512×512) and 3D grids (512×512×512) from 12,000+ simulated universes, stored as NumPy arrays optimized for ML training with gas, dark matter, and stellar properties at multiple redshifts.

**EAGLE simulations** (icc.dur.ac.uk/Eagle/database.php) from Durham University's Virgo Consortium provide 24 large-scale cosmological hydrodynamics runs with comprehensive galaxy formation physics spanning volumes of 25-100 Mpc containing over 1 million galaxies. Data access combines HDF5 particle data and halo catalogs with a sophisticated SQL database interface enabling complex queries on galaxy properties. The Millennium Simulation (wwwmpa.mpa-garching.mpg.de/galform/millennium), another Virgo Consortium landmark, traces 10 billion dark matter particles through a 500 Mpc/h box generating approximately 25 TB of data with 9 million semi-analytic galaxies accessible via Virtual Observatory SQL queries.

**Specialized datasets for machine learning** include the CosmoFlow collection from NERSC (portal.nersc.gov/project/m3363) with approximately 10,000 cosmological N-body simulations stored in HDF5 format. Each simulation provides 512³ particle histograms at 4 redshifts with varied cosmological parameters, generated using MUSIC and pyCOLA codes. Additional dark matter flow analysis datasets appear on Zenodo (zenodo.org/records/6757654 and zenodo.org/records/6541231) with halo-based and correlation-based statistics in HDF5 and CSV formats.

## Particle physics: Collision data from the world's largest experiments

**CERN's Open Data Portal** (opendata.cern.ch) provides unprecedented public access to over 5 petabytes of real collision data from Large Hadron Collider experiments including CMS, ATLAS, ALICE, and LHCb. The portal distributes data in ROOT files and HDF5 formats alongside detector simulations, analysis tools, and virtual machines for running analyses. CMS has released over 2 PB including 100% of research data from certain runs, with specialized machine learning datasets for particle identification and jet physics. ATLAS Open Data (opendata.atlas.cern) complements this with collision and simulated data, interactive Jupyter notebooks, and the Phoenix event display tool for visualizing Higgs and Z boson discovery events.

## Computational fluid dynamics: Turbulence and aerodynamics at scale

**The Johns Hopkins Turbulence Database** (turbulence.pha.jhu.edu, DOI: 10.17616/R3FK7B) represents the premier open-access turbulence resource with over 1 petabyte of direct numerical simulation and large eddy simulation data. The database contains multiple flow types including 100 TB of isotropic turbulence DNS, 130 TB of forced turbulent channel flow, 105 TB of transitional boundary layer data, and 40 TB of wind farm LES datasets. A landmark 32,768³ DNS dataset contributes approximately 0.5 PB alone. Data access occurs through web service APIs in Python, MATLAB, Fortran, and C, with a cutout service for downloading subsets in HDF5 or Zarr formats. The database enables researchers to analyze turbulence without requiring supercomputer access for simulation generation.

**Automotive aerodynamics datasets** support both engineering analysis and machine learning development. DrivAerNet++ (github.com/Mohamedelrefaie/DrivAerNet, Harvard Dataverse) provides the most comprehensive collection with 8,000 parametric car designs totaling 39 TB of data requiring 3 million CPU-hours on MIT Supercloud to generate. Each of the fastback, notchback, and estateback configurations includes 3D meshes (STL), surface pressure and wall shear stress, full volumetric CFD fields, point clouds, aerodynamic coefficients, semantic segmentation for 29 parts, and 2D renderings—all stored in HDF5, VTK, and Parquet formats under CC BY-NC 4.0 license. WindsorML (neilashton.github.io/caemldatasets/windsorml, also on HuggingFace) contains 355 Windsor body geometric variants with high-fidelity wall-modeled LES using approximately 300M cells per case, totaling 8 TB in VTU, STL, STEP, and CSV formats under CC BY-SA 4.0.

**Machine learning-focused CFD datasets** increasingly appear in Parquet format. AirfRANS (data.isir.upmc.fr/extrality/NeurIPS_2022) provides Reynolds-Averaged Navier-Stokes simulations over airfoils in Parquet format optimized for ML pipelines, with source code (github.com/Extrality/AirfRANS) and documentation (airfrans.readthedocs.io) under ODbL license. MegaFlow2D contains over 2 million snapshots from 3,000 different 2D fluid configurations with multi-fidelity data for super-resolution tasks, using PyTorch Geometric interfaces built on FEniCS/Oasis. The Curated Turbulence Modelling Dataset (nature.com/articles/s41597-021-01034-2, also on Kaggle) provides the first open-source dataset specifically designed for ML-augmented turbulence closure with 895,640 data points per model across k-ε, k-ω, k-ω SST, and k-ε-ϕt-f, stored as NumPy arrays and OpenFOAM case files with 47 tensor basis invariants.

## Molecular dynamics and protein simulations

**mdCATH** (open.playmolecule.org/mdcath, also on HuggingFace) represents the largest all-atom protein domain MD dataset with over 3 TB covering 5,398 CATH database domains simulated at 5 temperatures (320-450K) with 5 replicates each, totaling 134,950 trajectories capturing protein unfolding dynamics. Uniquely, this dataset includes instantaneous forces alongside coordinates from CHARMM22* force field calculations, with coordinates and forces sampled every 1 ns across over 62 milliseconds of accumulated simulation time. Data storage uses HDF5 as the primary format with XTC and PDB also available, including secondary structure (DSSP), RMSD, RMSF, and radius of gyration metrics.

**GPCRmd** (gpcrmd.org) provides a specialized community database for G-protein coupled receptor simulations from 23 international institutions, containing 556.5 microseconds across 371 systems covering 60% of available GPCR structures. The database includes 181 ligand-GPCR complexes and 190 apo systems with 3×500 ns per system, all following standardized protocols with web-based analysis tools and interactive visualization. **HDF5 format standards** for MD include MDTraj (mdtraj.org) with its HDF5 specification supporting topology embedding and flexible compression, and H5MD (nongnu.org/h5md) defining a standardized HDF5 hierarchy supported by LAMMPS, HOOMD, and other major MD packages.

## Genomics: Sequencing data at petabase scale

**The NCBI Sequence Read Archive** (ncbi.nlm.nih.gov/sra, AWS: registry.opendata.aws/ncbi-sra) serves as NIH's archive of high-throughput sequencing data, part of the International Nucleotide Sequence Database Collaboration with over 27 petabases as of 2019 across 9+ million records and continuously growing. The archive covers all branches of life plus metagenomic and environmental surveys, stored in BAM, CRAM, FASTQ, and SRA native formats with cloud access via AWS and Google Cloud enabling analysis without downloading complete datasets.

**10x Genomics HDF5 datasets** provide single-cell genomics and spatial transcriptomics data including feature-barcode matrices, molecule info files, and Visium HD feature slices at 2 μm resolution. Cell Ranger pipelines output structured HDF5 files with groups for barcodes, features, UMIs, and counts supporting single-cell RNA-seq, ATAC-seq, and spatial applications. The h5vc Bioconductor package demonstrates HDF5's genomics efficiency by compressing 21 human exomes from approximately 150 GB of BAM files to 6.5 MB HDF5 tallies, enabling cohort-scale variant calling and mutation spectrum analysis.

## Quantum chemistry and molecular properties

**QM9** (quantum-machine.org/datasets) established the benchmark for ML molecular property prediction with 133,885 equilibrium organic molecules containing up to 9 heavy atoms (C, N, O, F) from the GDB-17 database. Each molecule includes 13+ quantum chemical properties calculated at B3LYP/6-31G(2df,p) level including energies, harmonic frequencies, dipole moments, polarizabilities, and HOMO-LUMO gaps, distributed in XYZ, SDF, and database files. **ANI-1** (doi.org/10.6084/m9.figshare.c.3846712, github.com/isayev/ANI1_dataset) extends beyond equilibrium with approximately 20 million off-equilibrium conformations for 57,462 small organic molecules at ωB97x/6-31G(d) level, stored in HDF5 format exceeding 100 GB—providing 100× more data than QM9 for training transferable neural network potentials.

**Advanced quantum chemistry datasets** address multi-fidelity learning and molecular complexity. MultiXC-QM9 (data.dtu.dk/collections/MultiXC-QM9/6185986) computes QM9 molecules with 76 different DFT functionals and 3 basis sets yielding 228 energy values per molecule plus reaction energies for all monomolecular interconversions, enabling delta learning and transfer across methods. Hessian QM9 (nature.com/articles/s41597-024-04361-2) provides 41,645 molecules with full numerical Hessian matrices, vibrational frequencies, and normal modes at ωB97x/6-31G* in vacuum and 3 solvents, representing the first database with molecular Hessians essential for accurate vibrational frequency prediction. QO2Mol scales to 120,000 molecules with 20 million conformers supporting over 40 heavy atoms and elements H, C, N, O, F, P, S, Cl, Br, I with 40+ quantum properties at B3LYP/def2-SVP level. Aquamarine provides 59,783 conformers of 1,653 large drug-like molecules (mean 50.9 atoms, up to 92) with PBE0+MBD calculations in gas phase and implicit water.

## Materials science: Electronic structure and crystal properties

**The Materials Project** (materialsproject.org, next-gen.materialsproject.org, AWS: registry.opendata.aws/materials-project) provides the most comprehensive computed materials database with over 154,000 inorganic materials and 172,000+ molecules calculated using VASP density functional theory with GGA(+U) and R²SCAN functionals. The database maintained by Lawrence Berkeley National Laboratory includes formation energies corrected for accuracy (within experimental uncertainties), band structures, density of states, phase diagrams, elastic properties, and dielectric properties—all accessible via Python API (mp-api), web interface, and AWS S3 downloads in JSON, CIF, and POSCAR formats under open license with free registration.

**OQMD (Open Quantum Materials Database)** (oqmd.org) from Northwestern University contains 1.3+ million DFT-calculated materials from ICSD and prototype decorations with thermodynamic and structural properties at PBE level with GGA+U corrections, achieving 0.096 eV/atom mean absolute error versus experiments. The entire database downloads freely under CC-BY 4.0 with JSON API and OPTIMADE support. **AFLOW** (aflowlib.org, aflow.org) from Duke University represents the largest repository with 4+ million compounds including complete phase diagrams for over 650 binary systems, electronic band structures, and magnetic properties stored in AFLOW format, POSCAR, CIF, and JSON via REST API with OPTIMADE support. The AFLOW Prototype Encyclopedia categorizes all entries into 1,783+ crystallographic prototypes.

**NOMAD (Novel Materials Discovery)** (nomad-lab.eu, repository.nomad-coe.eu) serves as an open-source infrastructure with 19+ million materials science records from 40+ simulation codes including VASP, FHI-aims, Gaussian, and Quantum ESPRESSO. Maintained by Fritz Haber Institute's FAIRmat consortium, NOMAD stores data in HDF5 and JSON with automatic extraction following FAIR principles, including a specialized hybrid functional database with 7,024 materials calculated at HSE06 level—rare due to computational expense—providing electronic structure beyond standard GGA approximations. **JARVIS** (jarvis.nist.gov) from NIST contributes DFT calculations, force field parameters, and ML-ready datasets with benchmarking tools in JSON format.

## Protein structures and biomolecular databases

**The Protein Data Bank** (rcsb.org, wwpdb.org) maintains the global repository of 230,000+ experimentally determined 3D structures of proteins, nucleic acids, and complex assemblies from X-ray crystallography, NMR, and cryo-EM. Managed by the worldwide PDB consortium (RCSB, PDBe, PDBj, BMRB), the database provides weekly updates in PDB legacy format, PDBx/mmCIF current standard, and PDBML XML formats—all in CC0 public domain with FTP download and APIs. **AlphaFold Protein Structure Database** (alphafold.ebi.ac.uk) complements experimental structures with over 200 million AI-predicted protein structures from DeepMind in PDB and mmCIF formats.

**PubChem** (pubchem.ncbi.nlm.nih.gov) represents the world's largest free chemical information resource with 111 million+ compounds, 293 million+ substances, and 1.25 million+ bioassays. NCBI/NIH maintains this comprehensive database with chemical structures, physical properties, biological activities, safety data, patents, and literature in SDF, SMILES, InChI, XML, and JSON formats with FTP bulk download (ftp.ncbi.nlm.nih.gov/pubchem) and PubChem3D for spatial structures.

## General-purpose scientific data repositories

**Zenodo** (zenodo.org), operated by CERN, serves as a general-purpose repository supporting datasets up to 50 GB free with larger sizes via Zenodo+, accumulating over 45 million views and 55 million downloads. Notable HPC datasets include M100 ExaData with 49.9 TB from CINECA's Marconi100 supercomputer (nature.com/articles/s41597-023-02174-3), F-DATA with 24 million Fugaku job traces (nature.com/articles/s41597-025-05633-1), and Globus Compute with 2.1M task submissions (zenodo.org/records/10044780). All uploads receive automatic DOI assignment with support for all file formats under various licenses including CC0 and CC-BY.

**Figshare** (figshare.com) provides open access for research outputs with free hosting up to 20GB and Figshare+ for larger datasets (100GB to 5TB+), using Amazon S3 storage with CLOCKSS backup and expert curation teams for large deposits. Both platforms enable FAIR data sharing with permanent identifiers, though Zenodo's CERN backing and unlimited size support (with approval) often makes it preferred for very large HPC datasets.

**DOE Open Energy Data Initiative** (nrel.gov, OEDI) hosts a cloud-based Data Lake making high-value energy research datasets accessible and AI-ready, including NREL PV Rooftop Data, the National Solar Radiation Database, Wind Integration National Dataset Toolkit, and DOE Water Power Technology wave datasets. The infrastructure co-locates compute with storage for efficient analysis of multi-terabyte energy datasets.

## Accessing and using these datasets effectively

Most major repositories provide multiple access methods optimized for different use cases. **NASA Earthdata** offers direct HTTPS downloads, OPeNDAP remote subsetting avoiding full file downloads, THREDDS Data Servers with catalog browsing, and cloud access via AWS S3 for analysis near the data. Python tools like podaac-data-downloader streamline bulk transfers. **Climate data** from CMIP6 uses the Earth System Grid Federation with Globus high-performance transfers, wget scripts, and Python clients. ECMWF provides the ecmwf-opendata Python package and Copernicus Climate Data Store API. **Materials databases** offer REST APIs: Materials Project's mp-api, AFLOW's JSON endpoints, and OQMD's downloadable database files all with OPTIMADE standard support for federated queries.

**Format selection significantly impacts I/O performance.** HDF5 and NetCDF-4 support parallel I/O critical for HPC applications, internal compression reducing storage and transfer costs, and chunked storage optimizing different access patterns. NetCDF-4 files are HDF5 files with CF conventions ensuring metadata consistency and tool interoperability. ADIOS2 specifically targets exascale computing with streaming support and in-situ capabilities. For cloud-native workflows, newer formats like Zarr (used in JHTDB recent datasets and CESM on AWS) optimize for object storage access patterns. Traditional HPC applications should prefer HDF5/NetCDF-4/ADIOS2, while ML pipelines may benefit from Parquet's columnar storage for large-scale analytics—though this remains less common in scientific computing than business intelligence applications.

**Size considerations range dramatically.** Petabyte-scale resources include IllustrisTNG (1.1 PB), CERN Open Data (5+ PB), NCBI SRA (27+ petabases), and the complete CMIP6 archive (hundreds of TB distributed). Terabyte-scale datasets like ERA5 (390 TB for model levels), CAMELS (200+ TB), DrivAerNet++ (39 TB), and individual climate model ensembles suit many research projects. Research-ready subsets from gigabytes to single-digit terabytes enable rapid prototyping without infrastructure investments, with most repositories offering subsetting services to extract only needed spatiotemporal regions or variables before download.

## Conclusion: A mature ecosystem for data-driven science

This comprehensive survey identifies a mature ecosystem of openly accessible scientific datasets spanning every major HPC domain. **National laboratories maintain the most authoritative and stable resources**: Argonne's continuously updated Darshan traces from production systems, Oak Ridge's ADIOS2 development and examples, NCAR's 500+ TB of climate model output, and DOE facilities' energy datasets. **International collaborations** produce landmark resources like CMIP6's federated climate projections, CERN's 5-petabyte particle physics archive, and the worldwide PDB consortium's 230,000+ protein structures. **University research groups** contribute specialized high-value datasets including Johns Hopkins' petabyte turbulence database, MIT's 39 TB DrivAerNet++ automotive aerodynamics collection, and Northwestern's 1.3-million-material OQMD database.

The convergence on standardized formats—particularly HDF5 and its derivatives NetCDF-4 and HDF-EOS5—enables robust tool ecosystems spanning NCO operators, Xarray for Python, and domain-specific analysis packages. Cloud hosting increasingly complements traditional download mechanisms, with major datasets appearing on AWS Open Data (ERA5, CMIP6, CESM, Materials Project, NCBI SRA) and facilitating analysis-near-data workflows. This extensive catalog provides HPC developers with realistic I/O workloads for system benchmarking while offering domain scientists immediate access to high-quality data that would otherwise require millions of CPU-hours and specialized expertise to generate.